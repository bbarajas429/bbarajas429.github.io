[
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Visualizing My Personal Spending Data Using R-Shiny\n\n\n\n\n\n\nR\n\n\nVisualization\n\n\nDashboard\n\n\n\nUsing R-Shiny to re-create the Mint (by Intuit) interactive dashboard\n\n\n\n\n\nJun 20, 2024\n\n\nBriana Barajas\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an infographic using Oregon spotted frog capture data\n\n\n\n\n\n\nMEDS\n\n\nR\n\n\nVisualization\n\n\n\nCreating custom data visualizations in R using mark-recapture data from USGS\n\n\n\n\n\nMar 12, 2024\n\n\nBriana Barajas\n\n\n\n\n\n\n\n\n\n\n\n\nNDVI Assessment Around Santa Clara River\n\n\n\n\n\n\nMEDS\n\n\nR\n\n\nRemote sensing\n\n\n\nCalculating NDVI using Landsat remote sensing data around the Santa Clara River area\n\n\n\n\n\nDec 15, 2023\n\n\nBriana Barajas\n\n\n\n\n\n\n\n\n\n\n\n\nAir Quality Assesment of California Thomas Fire\n\n\n\n\n\n\nPython\n\n\nMEDS\n\n\nRemote sensing\n\n\n\nAn analysis of the Thomas Fire, including a raster image of the fire’s perimeter and plots depicting the air quality (AQI) in affected counties.\n\n\n\n\n\nDec 13, 2023\n\n\nBriana Barajas\n\n\n\n\n\n\n\n\n\n\n\n\nDrought Tolerance in Tropical Tree Species\n\n\n\n\n\n\nMEDS\n\n\nR\n\n\nStatistics\n\n\n\nComparison of linear models, summarizing tropical tree growth during a severe drought\n\n\n\n\n\nDec 10, 2023\n\n\nBriana Barajas\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog-posts/2023-12-15-santa-clara-ndvi/index.html",
    "href": "blog-posts/2023-12-15-santa-clara-ndvi/index.html",
    "title": "NDVI Assessment Around Santa Clara River",
    "section": "",
    "text": "While many organisms in nature experience phenological cycles, plant phenology is one of the most easily recognized. Phenological events for plants include leaf growth, flowering, and leaf death (also known as senescence). The timing of these events is based on climate conditions, so that plants may ensure successful reproduction. As the climate changes, phenological cycles in plants can be disrupted. For this reason, changes in plant phenology are used to estimate how ecosystems are responding to climate change.\nThis analysis utilizes satellite data to calculate the normalized difference vegetation index (NDVI) for various plant communities. Looking at NDVI over time can uncover some of the phenological cycles, but more importantly, long-term trends that might be caused by climate change. The area of interest for this study is the Santa Clara River, which has the following plant communities:\n\nriparian forests: grow along the river, dominated by winter deciduous cottonwood and willow trees\ngrasslands: grow in openspaces, dominated by drought deciduous grasses\nchaparral shrublands: grow in more arid habitats, dominated by evergreen shrubs\n\ncredit: this post is based on a materials developed by Chris Kibler with additional assistance from Ruth Oliver"
  },
  {
    "objectID": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#background",
    "href": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#background",
    "title": "NDVI Assessment Around Santa Clara River",
    "section": "",
    "text": "While many organisms in nature experience phenological cycles, plant phenology is one of the most easily recognized. Phenological events for plants include leaf growth, flowering, and leaf death (also known as senescence). The timing of these events is based on climate conditions, so that plants may ensure successful reproduction. As the climate changes, phenological cycles in plants can be disrupted. For this reason, changes in plant phenology are used to estimate how ecosystems are responding to climate change.\nThis analysis utilizes satellite data to calculate the normalized difference vegetation index (NDVI) for various plant communities. Looking at NDVI over time can uncover some of the phenological cycles, but more importantly, long-term trends that might be caused by climate change. The area of interest for this study is the Santa Clara River, which has the following plant communities:\n\nriparian forests: grow along the river, dominated by winter deciduous cottonwood and willow trees\ngrasslands: grow in openspaces, dominated by drought deciduous grasses\nchaparral shrublands: grow in more arid habitats, dominated by evergreen shrubs\n\ncredit: this post is based on a materials developed by Chris Kibler with additional assistance from Ruth Oliver"
  },
  {
    "objectID": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#about-the-data",
    "href": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#about-the-data",
    "title": "NDVI Assessment Around Santa Clara River",
    "section": "About the Data",
    "text": "About the Data\n\nRiver Data\nA shape file of rivers in Ventura County is publicly available via the Ventura County Watershed Protection District. To provide a base map for this, I have also loaded vector data on counties in the United States.\n\n\nLandsat Data\nThe data that will be used to calculate NDVI is from the Landsat Operational Land Imager (OLI) sensor. There are 8 total landsat .tif files that contain level 3 surface reflectance products where erroneous values were set to NA, the scale factor is set to 100, and bands 2-7 are present. The date of collection is at the end of each file name.\n\n\nStudy Sites\nStudy sites are available as vector data with character strings as the plant type. This data will be used to classify the plant communities of interest."
  },
  {
    "objectID": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#compute-initial-ndvi",
    "href": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#compute-initial-ndvi",
    "title": "NDVI Assessment Around Santa Clara River",
    "section": "Compute Initial NDVI",
    "text": "Compute Initial NDVI\nNDVI was computed for all 8 Landsat images. The date range is between June 2018 and July 2019. Ideally, this time frame will provide an accurate estimate of annual phonological changes in NDVI.\nPreparation:\n\n\nView Code\n# create NDVI function\nndvi_fun = function(nir, red){\n  (nir - red) / (nir + red)\n}\n\n# create list of landsat files\nlandsat_files &lt;- list.files(here('blog-posts','2023-12-15-santa-clara-ndvi',\"data\"), pattern = \"*.tif\", full.names = TRUE)\n\n\nNDVI Across Scenes:\nIn order to facilitate NDVI calculation, the function below is designed to read in all raster files, rename their bands, and calculate NDVI. Once all NDVI’s were calculated, they were stacked and assigned names that corresponded to their date.\n\n\nView Code\n#function reads in data, renames bands, and calc NDVI\ncreate_ndvi_layer &lt;- function(i){\n  landsat &lt;- rast(landsat_files[i]) #read in data \n  names(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\") #rename bands\n  ndvi &lt;- lapp(landsat[[c(4, 3)]], fun = ndvi_fun) #NDVI\n}\n\n# stack raster with NDVI data using function above\nall_ndvi &lt;- c(create_ndvi_layer(1),\n              create_ndvi_layer(2),\n              create_ndvi_layer(3),\n              create_ndvi_layer(4),\n              create_ndvi_layer(5),\n              create_ndvi_layer(6),\n              create_ndvi_layer(7),\n              create_ndvi_layer(8))\n\n# add dates to corresponding layer\nnames(all_ndvi) &lt;- c(\"2018-06-12\",\n                     \"2018-08-15\",\n                     \"2018-10-18\",\n                     \"2018-11-03\",\n                     \"2019-01-22\",\n                     \"2019-02-23\",\n                     \"2019-04-12\",\n                     \"2019-07-01\")"
  },
  {
    "objectID": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#exploratory-maps",
    "href": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#exploratory-maps",
    "title": "NDVI Assessment Around Santa Clara River",
    "section": "Exploratory Maps",
    "text": "Exploratory Maps\n\nTrue Color Image\nAssessing true color imagery before NDVI can be useful for quickly recognizing different types of terrain. Here, I created a simple true color image of the Santa Clara River area using one of a single Landsat from June 2018.\n\n\nView Code\n# create a true color image\nplotRGB(single_landsat_rast, 3, 2, 1, stretch=\"hist\",\n        main = \"Santa Clara River True Color Image\")\n\n\n\n\n\n\n\n\n\n\n\nStudy Site Location Along Santa Clara Watershed\nThe Santa Clara River flows from Santa Clarita to Ventura and remains relatively natural compared to other rivers in California. It is recognized for its ecological importance and natural resource potential, as the river provides water for agriculture and habitat for several endangered species. For these reasons, there have been significant efforts to conserve and restore the riparian habitat surrounding the river1.\nThe following visualization depicts the Santa Clara River, as well as other rivers within the Santa Clara Watershed. The plant communities (study sites) were added as well for geographical context.\n\n\nView Code\n## ========== Data Preparation ==========\n# filter VC river data to SC watershed\nsc_watershed &lt;- vc_rivers %&gt;% \n  filter(WATERSHED == \"SANTA CLARA RIVER WATERSHED\") %&gt;% #filter\n  st_transform(crs = 'epsg:32611') %&gt;% #reproject\n  st_crop(study_sites)\n\n# filter state data to counties new watershed\nventura &lt;- states %&gt;% st_crop(study_sites) #crop county to watershed\n\n# create custom color palette\nphenology_pal &lt;- c(\"#EAAC8B\", \"#315C2B\", \"#315C2B\", \"#315C2B\",\"#9EA93F\")\n\n## ========== Plot Data Together ==========\n# create map combining watershed, study sites, and counties\nggplot() +\n  geom_sf(data = ventura, fill = '#E7DED9') +\n  geom_sf(data = sc_watershed, color = 'dodgerblue') +\n  geom_sf(data = study_sites,\n          mapping = aes(fill = study_site)) +\n  scale_fill_manual(values = phenology_pal) +\n  labs(title = \"Study Sites Along Santa Clara Watershed\",\n       fill = \"Plant Communities\",\n       x=NULL,\n       y=NULL) +\n  theme(panel.background = element_rect(fill='lightblue'),\n        plot.margin=grid::unit(c(0,0,0,0), \"mm\"))\n\n\n\n\n\n\n\n\n\n\n\nStudy Sites and NDVI\nThis map demonstrates study sites within a single NDVI layer. For simplicity, I will call the first layer from the all_ndvi raster stack.\n\n\nView Code\ntm_shape(all_ndvi[[1]]) +\n  tm_raster() +\n  tm_shape(study_sites) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\nThe normalized vegetation index (NDVI) can be used to estimate plant productivity. Areas with sparse foliage will return lower values, while areas with dense leaf cover will return values closer to 1. The NDVI scale ranges from -1 to 1, but vegetation is commonly a positive value. A negative NDVI is indicative of water, either as clouds, snow, or bodies of water on the Earth’s surface. For this reason, cloud cover can potentially affect NDVI values calculated from remotely sensed data."
  },
  {
    "objectID": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#ndvi-within-study-sites",
    "href": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#ndvi-within-study-sites",
    "title": "NDVI Assessment Around Santa Clara River",
    "section": "NDVI Within Study Sites",
    "text": "NDVI Within Study Sites\n\nData Cleaning\nBefore the final plot can be produced, the average NDVI within each study site must be isolated. This was done using the extract() function from the terra package, followed by cbind to combine NDVI values with the original study site data.\n\n\nView Code\n## ========== Find NDVI within sites ==========\n# use extract to pull NDVI values\nsites_ndvi &lt;- terra::extract(all_ndvi, study_sites, fun = 'mean')\n\n# bind site data w/ site specific NDVI\nsites_ndvi_raw &lt;- cbind(study_sites, sites_ndvi)\n\n## ========== Clean Data frame =========\nsites_clean &lt;- sites_ndvi_raw %&gt;% \n  st_drop_geometry() %&gt;%\n  select(-ID) %&gt;%\n  pivot_longer(!study_site) %&gt;%\n  rename(\"NDVI\" = value) %&gt;%\n  mutate(\"year\" = str_sub(name, 2, 5),\n         \"month\" = str_sub(name, 7, 8),\n         \"day\" = str_sub(name, -2, -1)) %&gt;%\n  unite(\"date\", 4:6, sep = \"-\") %&gt;%\n  mutate(\"date\" = lubridate::as_date(date))\n\n\n\n\nPlot NDVI Over Time\nPlotting NDVI over time can ideally give us insight into the phonological trends mentioned in the introduction. Typically, NDVI values will be lower in the winter when trees lose their leaves, and higher in the spring. Plotting all study sites together makes it easy to note differences in NDVI among different plant communities.\n\n\nView Code\nggplot(sites_clean,\n       aes(x = date, y = NDVI,\n           group = study_site, col = study_site)) +\n  scale_color_manual(values = phenology_pal) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"\", y = \"Normalized Difference Vegetation Index (NDVI)\", col = \"Plant Communities\",\n       title = \"Seasonal cycles of vegetation productivity\")"
  },
  {
    "objectID": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#conclusion",
    "href": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#conclusion",
    "title": "NDVI Assessment Around Santa Clara River",
    "section": "Conclusion",
    "text": "Conclusion\nHigher NDVI values often correlate with denser vegetation and times of rapid growth. Based on the plot produced, grasslands and chaparral sites appear to have faster growth in the springtime. Most notably, however, it appears that grasslands are ill equipped for winter as the NDVI from the end of 2018 is rather low. To expand this analysis, it would be useful to data beyond just one year. It is possible this year was not an accurate representation, and additional data would uncover this. With enough data, you could also minimize seasonal cycles to uncover trends that are only visible over several years."
  },
  {
    "objectID": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#footnotes",
    "href": "blog-posts/2023-12-15-santa-clara-ndvi/index.html#footnotes",
    "title": "NDVI Assessment Around Santa Clara River",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Santa Clara River.” The Nature Conservancy, www.nature.org/en-us/get-involved/how-to-help/places-we-protect/the-nature-conservancy-in-california-santa-clara-river-california-con/#:~:text=The%20Santa%20Clara%20River%20is%20a%20vital%20source%20of%20drinking,bustling%20Los%20Angeles%2DVentura%20region. Accessed 15 Dec. 2023.↩︎"
  },
  {
    "objectID": "blog-posts/2023-12-10-luquillo-drought/index.html",
    "href": "blog-posts/2023-12-10-luquillo-drought/index.html",
    "title": "Drought Tolerance in Tropical Tree Species",
    "section": "",
    "text": "View Code\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(lubridate)\n\nlibrary(gt)\nlibrary(broom)\n\nlibrary(feasts)\nlibrary(tsibble)\nlibrary(zoo)\n\ndata_path <- here(\"blog-posts\", \"2023-12-10-luquillo-drought\", \"data\")\n\n# tree diameter at breast height\ndbh_raw <- read_csv(here(data_path, \"long.data.DBH.csv\"))\n\n# hourly climate data \nclim_1999_2014 <- read_csv(here(data_path, \"NADPTowerHourlyData1999_2014.csv\")) %>% \n  clean_names()\nclim_2015_2023 <- read_csv(here(data_path, \"NADPTowerHourlyData2015_2023v2.csv\")) %>% \n  clean_names()"
  },
  {
    "objectID": "blog-posts/2023-12-10-luquillo-drought/index.html#motivation",
    "href": "blog-posts/2023-12-10-luquillo-drought/index.html#motivation",
    "title": "Drought Tolerance in Tropical Tree Species",
    "section": "Motivation",
    "text": "Motivation\nClimate change, in conjunction with other environmental stressors, continue to threaten forests around the world. Mass tree deaths, fires, and deforestation can create a negative feedback loop, turning these well-recognized carbon sinks into a carbon source. Forests are also recognized for their role in preventing erosion, filtering air, and providing habitat and recreation.\nDroughts have been proven to adversely affect forest health. Minimal and inconsistent water supply affects phenology and increases the chance of insect outbreaks or wildfires1. While the severity of these threats is not novel to me, I recognized that most of my knowledge on the topic was based on studies conducted in temperate forests. I was curious to learn more about how droughts impacted tropical species, which are less adapted to severe weather fluctuations."
  },
  {
    "objectID": "blog-posts/2023-12-10-luquillo-drought/index.html#goal",
    "href": "blog-posts/2023-12-10-luquillo-drought/index.html#goal",
    "title": "Drought Tolerance in Tropical Tree Species",
    "section": "Goal",
    "text": "Goal\nTo conduct a statistical assessment of tropical tree growth under drought conditions. This was accomplished using a series of linear regressions to model the relationship between tree growth and several climate variables. Using these regressions, I sought to examine if tree species with a widespread familial distribution (left) were better prepared for droughts than species with narrow familial distribution (right).\n\n\n\nDistribution data from Global Biodiversity Information Facility (GBIF)"
  },
  {
    "objectID": "blog-posts/2023-12-10-luquillo-drought/index.html#data",
    "href": "blog-posts/2023-12-10-luquillo-drought/index.html#data",
    "title": "Drought Tolerance in Tropical Tree Species",
    "section": "Data",
    "text": "Data\n\nAbout the Data\nData of tree diameter over time is publicly available from the DataONE repository. This dataset includes measures of tree diameter at breast height (dbh) for several tropical tree species in the Luquillo National Research Forest in Puerto Rico. The Luquillo Forest is part of the Long-Term Ecological Research Network (LTER), so substantial meteorological data for the area was also available. The diameter measurements were taken during a severe drought, lasting from 2013 to 2016.\nAlthough there were several climate variables available, I reduced the number of variables in the models to minimize the possibility of over fitting. Mean average temperature and total rainfall were included to summarize the effects of drought. Photosynthetic photon flux density (ppfd) was also included. PPFD is a measurement of the number of photons between 400 and 700 nm in wavelength, and how frequently these waves this the leaf’s surface. This wavelength range is optimal for plant molecules to absorb, thus ppfd acts as a measurement of energy available for plants.2\n\n\nTree Species\nIn an effort to minimize error, modeling was limited to the four most sampled tree species. Species were also selected based on their taxonomic family, and whether that family had a wide or narrow distribution. Species and family information was provided in the metadata, but I will summarize it here:\n\n\n\n\n\n\n\n\n\nAbbreviation\nSpecies\nFamily\nFamily Distribution\n\n\n\n\nDACEXC\nDacryodes excelsa\nBurseraceae\nnarrow\n\n\nMANBID\nManilkara bidentata\nSapotaceae\nnarrow\n\n\nCASARB\nCasearia arborea\nSalicaceae\nwide\n\n\nINGLAU\nInga laurina\nFabaceae\nwide\n\n\n\n\n\nData Cleaning\nAlthough data collection for tree diameters began in 2011, measurements were not consistently taken until June 2014. In order to create time series models without significant data gaps, I restricted the data from June 2014 to July 2016, when diameter data was collected monthly.\n\n\nView Code\n## =====================================\n##           Clean Tree Data        ----\n## =====================================\n\ndbh <- dbh_raw %>% \n  \n  # correct date object format\n  mutate(date = as.Date(paste(year, doy, sep = \"-\"), \"%Y-%j\")) %>% \n  \n  # filter to desired species\n  filter(species %in% c(\"DACEXC\", \"MANBID\", \"CASARB\", \"INGLAU\")) %>% \n  \n  # calculate diameter averages per species\n  select(-c(\"doy\", \"year\", \"flag\")) %>% \n  group_by(date, species) %>% \n  summarise(mean_daily_dbh = mean(dbh, na.rm = TRUE)) %>% \n  ungroup() %>% \n  \n  # filter to desired time range\n  filter(date(date) >= \"2014-06-01\" & date(date) < \"2016-08-01\")\n  \n# add family and distribution information\ndbh <- dbh %>% \n  mutate(family = case_when(species == 'CASARB' ~ 'Salicaceae',\n                            species == 'MANBID' ~ 'Sapotaceae',\n                            species == 'DACEXC' ~ 'Burseraceae',\n                            species == 'INGLAU' ~ 'Fabaceae'),\n         distribution = case_when(family == 'Salicaceae' ~ 'wide',\n                                  family == 'Sapotaceae' ~ 'narrow',\n                                  family == 'Burseraceae' ~ 'narrow',\n                                  family == 'Fabaceae' ~ 'wide'),\n         distribution = as.factor(distribution))\n\n\nClimate data that was collected daily had far more missing values than data collected hourly, so I used hourly climate data to calculate daily and monthly averages.\n\n\nView Code\n## =====================================\n##           Clean Climate Data     ----\n## =====================================\n\n# select and rename climate variables (1999-2014)\nclim_1999_2014 <- clim_1999_2014 %>% \n  mutate(datetime = mdy_hm(datetime)) %>% \n  filter(date(datetime) >= \"2014-06-01\" & date(datetime) != \"2015-01-01\") %>% \n  select(c(\"datetime\", \"rain_mm\", \"temp_air_degrees_c\", \"ppfd_millimoles_m2_hour\")) %>% \n  rename(\"temp_c\" = \"temp_air_degrees_c\", \n         \"ppfd_mmol_m2_hour\" = \"ppfd_millimoles_m2_hour\")\n\n# select and rename climate variables (2015-2023)\nclim_2015_2023 <- clim_2015_2023 %>% \n  mutate(datetime = ymd_hms(datetime)) %>% \n  filter(date(datetime) < \"2016-08-01\") %>% \n  select(c(\"datetime\", \"rain_mm_tot\", \"air_tc_avg\", \"par_tot\")) %>% \n  rename(\"rain_mm\" = \"rain_mm_tot\",\n         \"temp_c\" = \"air_tc_avg\",\n         \"ppfd_mmol_m2_hour\" = \"par_tot\")\n\n# bind to combine study time (June 2014 - July 2016)\nhourly_conditions <- rbind(clim_1999_2014, clim_2015_2023) %>% \n  mutate(year_mo = yearmonth(datetime))\n\n## =====================================\n##         Calculate Averages       ----\n## =====================================\n# convert hourly to daily averages\ndaily_conditions <- hourly_conditions %>% \n  group_by(date = date(datetime)) %>% \n  summarise(tot_rain_mm = sum(rain_mm, na.rm = TRUE),\n            avg_temp_c = mean(temp_c, na.rm = TRUE),\n            avg_ppfd_mmol_m2 = mean(ppfd_mmol_m2_hour, na.rm = TRUE)) %>% \n   mutate(year_mo = yearmonth(date))\n\n# create monthly conditions\nmonthly_conditions <- hourly_conditions %>% \n  group_by(year_mo) %>% \n  summarise(tot_rain_mm = sum(rain_mm, na.rm = TRUE),\n            avg_temp_c = mean(temp_c, na.rm = TRUE),\n            avg_ppfd_mmol_m2 = mean(ppfd_mmol_m2_hour, na.rm = TRUE))\n\n# replace zeros w/NA, no data collected October 2014\nmonthly_conditions['tot_rain_mm'][monthly_conditions['tot_rain_mm'] == 0] <- NA\n\n# replace NAs with the mean of previous and next month\nmonthly_conditions$tot_rain_mm <- na.approx(monthly_conditions$tot_rain_mm)\n\n# remove raw data variables\nrm(clim_1999_2014, clim_2015_2023, dbh_raw, hourly_conditions)\n\n# create fully joined df\nclim_dbh_full <- left_join(dbh, daily_conditions, by = c(\"date\"))\n\n\n\n\nExploration - Climate Trends\nThe following plots show time series decomposition from June 2014 to July 2016 for average monthly temperature and total monthly rainfall. The seasonal_year plot suggest that seasonality has a high contribution to changes in rainfall and temperature. By removing seasonal fluctuations, you can view the overall trend exhibited by the climate variables. The increase in temperature and decrease in rainfall display the symptoms of drought.\n\nRainfall DecompositionTemperature Decomposition\n\n\n\n\nView Code\n# decompose monthly rain variable\nrain_dcmp <- monthly_conditions %>% \n  as_tsibble(index = year_mo) %>%\n  model(STL(tot_rain_mm))\n\n# plot \ncomponents(rain_dcmp) %>% autoplot()\n\n\n\n\n\n\n\n\n\nView Code\n# decompose monthly average temperature\ntemp_dcmp <- monthly_conditions %>% \n  as_tsibble(index = year_mo) %>% \n  model(STL(avg_temp_c))\n\n# plot components\ncomponents(temp_dcmp) %>% autoplot()\n\n\n\n\n\n\n\n\n\n\nExploration - Diameter Trends\nDiameter trends for all species selected appear linear overall. Some species experienced faster relative growth, particularly CASARB and MANBID which had smaller diameters overall.\n\n\nView Code\nggplot(data = dbh, aes(x=date, y= mean_daily_dbh, col = distribution)) +\n  geom_point() +\n  scale_color_manual(values = c('#3714ab', '#14A6AB'))+\n  labs(y = \"Diameter at Breast Height (mm)\",\n       title = \"Mean Diameter of Species\") +\n  guides(color = guide_legend(title = \"Distribution\")) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  facet_wrap(~species, scales = \"free_y\") +\n  theme(axis.title.x = element_blank())"
  },
  {
    "objectID": "blog-posts/2023-12-10-luquillo-drought/index.html#analysis---static-time-series",
    "href": "blog-posts/2023-12-10-luquillo-drought/index.html#analysis---static-time-series",
    "title": "Drought Tolerance in Tropical Tree Species",
    "section": "Analysis - Static Time Series",
    "text": "Analysis - Static Time Series\nIn order to summarize the results I was most interested in, I developed the following function:\n\n\nView Code\n# create function to run model and clean results\ntidy_results_fun <- function(model, data){\n  \n  # clean model outputs\n  tidy <- broom::tidy(model)\n  summary <- broom::glance(model)\n  \n  # combine desired results (keeping R/R^2 only once)\n  tidy$adj_r_squared <- ifelse(1:nrow(tidy) == 1, summary$adj.r.squared, NA)\n  tidy$r_squared <- ifelse(1:nrow(tidy) == 1, summary$r.squared, NA)\n  \n  # save results to environment\n  assign(paste0(tolower(unique(data$species)), \"_model\"), tidy, envir = .GlobalEnv)\n  \n}\n\n\n\nSimple linear regression\nA simple time series regression looking at tree diameter (y) over time (x) for the four species was conducted first. This can be written out mathematically as:\\[ \\hat{y} = \\beta_0 + \\beta_1 x_1  \\]\nWhere \\(\\beta_1\\) is the average change in diameter given a one unit change in time (\\(x_1\\)), and \\(\\beta_0\\) is the estimated diameter when time (\\(x_1\\)) is zero.\n\n\nView Code\n# apply function to all species\nfor (i in unique(clim_dbh_full$species)) {\n  data <- clim_dbh_full %>% filter(species == i)\n  model <- lm(mean_daily_dbh ~ date, data = data)\n  tidy_results_fun(model, data)\n}\n\n# combine species based on distributions\nwide <- rbind(inglau_model, casarb_model) %>% gt()\nnarrow <- rbind(dacexc_model, manbid_model) %>% gt()\n\n# view for wide distribution\nwide %>% \n  tab_header(\"Wide Distribution\") %>% \n  tab_row_group(label = \"I. laurina\", rows = 1:2) %>% \n  tab_row_group(label = \"C. arborea\", rows = 3:4) %>% \n  tab_options(row_group.background.color = \"grey90\")\n\n\n\n\n\n\n  \n    \n      Wide Distribution\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      adj_r_squared\n      r_squared\n    \n  \n  \n    \n      C. arborea\n    \n    (Intercept)\n-55.41282623\n2.007586e+01\n-2.7601721\n8.456750e-03\n0.70391672\n0.71064589\n    date\n0.01244958\n1.211459e-03\n10.2765188\n3.747518e-13\nNA\nNA\n    \n      I. laurina\n    \n    (Intercept)\n-32.27795546\n1.112263e+02\n-0.2902007\n7.730576e-01\n0.05806837\n0.07947591\n    date\n0.01293233\n6.711850e-03\n1.9267901\n6.063085e-02\nNA\nNA\n  \n  \n  \n\n\n\n\nView Code\n# view results for narrow distribution\nnarrow %>% \n  tab_header(\"Narrow Distribution\") %>% \n  tab_row_group(label = \"D. excelsa\", rows = 1:2) %>% \n  tab_row_group(label = \"M. bidentata\", rows = 3:4) %>% \n  tab_options(row_group.background.color = \"grey90\")\n\n\n\n\n\n\n  \n    \n      Narrow Distribution\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      adj_r_squared\n      r_squared\n    \n  \n  \n    \n      M. bidentata\n    \n    (Intercept)\n1.375030e+02\n8.129896e+00\n16.913252\n1.287841e-20\n0.12825930\n0.1480716\n    date\n1.341184e-03\n4.905911e-04\n2.733811\n9.053291e-03\nNA\nNA\n    \n      D. excelsa\n    \n    (Intercept)\n2.224853e+02\n2.421531e+01\n9.187794\n1.058230e-11\n0.09937944\n0.1198481\n    date\n3.535865e-03\n1.461251e-03\n2.419753\n1.983184e-02\nNA\nNA\n  \n  \n  \n\n\n\n\n\n\nMultiple Linear Regression\nThis regression compares diameter over time, adding all climate variables as predictors to see if the models predictive capability improves. This can be written out mathematically as:\\[ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4\\]\nWhere \\(x_2\\) through \\(x_4\\) are the added climate variables.\n\n\nView Code\n# apply function to all species (now using multiple regression)\nfor (i in unique(clim_dbh_full$species)) {\n  data <- clim_dbh_full %>% filter(species == i)\n  model <- lm(mean_daily_dbh ~ date + tot_rain_mm + avg_temp_c + avg_ppfd_mmol_m2, data = data)\n  tidy_results_fun(model, data)\n}\n\n# combine species based on distributions\nwide <- rbind(inglau_model, casarb_model) %>% gt()\nnarrow <- rbind(dacexc_model, manbid_model) %>% gt()\n\n# view for wide distribution\nwide %>% \n  tab_header(\"Wide Familial Distribution\") %>% \n  tab_row_group(label = \"I. laurina\", rows = 1:2) %>% \n  tab_row_group(label = \"C. arborea\", rows = 3:4) %>% \n  tab_options(row_group.background.color = \"grey90\")\n\n\n\n\n\n\n  \n    \n      Wide Familial Distribution\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      adj_r_squared\n      r_squared\n    \n  \n  \n    \n      C. arborea\n    \n    tot_rain_mm\n2.340958e-02\n5.056259e-02\n0.4629823\n6.461638e-01\nNA\nNA\n    avg_temp_c\n-2.451045e-01\n1.332287e+00\n-0.1839728\n8.550672e-01\nNA\nNA\n    \n      I. laurina\n    \n    (Intercept)\n4.783913e+01\n1.433299e+02\n0.3337693\n7.404901e-01\n0.03878215\n0.1349039\n    date\n8.026665e-03\n8.237144e-03\n0.9744476\n3.363370e-01\nNA\nNA\n    \n      \n    \n    avg_ppfd_mmol_m2\n7.171309e-03\n4.655729e-03\n1.5403194\n1.322274e-01\nNA\nNA\n    (Intercept)\n-6.820285e+01\n2.488952e+01\n-2.7402240\n9.493995e-03\n0.70085629\n0.7307707\n    date\n1.253374e-02\n1.430396e-03\n8.7624280\n1.872024e-10\nNA\nNA\n    tot_rain_mm\n-1.407233e-03\n8.780290e-03\n-0.1602719\n8.735629e-01\nNA\nNA\n    avg_temp_c\n4.870840e-01\n2.313541e-01\n2.1053612\n4.230170e-02\nNA\nNA\n    avg_ppfd_mmol_m2\n-1.869582e-04\n8.084762e-04\n-0.2312476\n8.184318e-01\nNA\nNA\n  \n  \n  \n\n\n\n\nView Code\n# view results for narrow distribution\nnarrow %>% \n  tab_header(\"Wide Familial Distribution\") %>% \n  tab_row_group(label = \"D. excelsa\", rows = 1:5) %>% \n  tab_row_group(label = \"M. bidentata\", rows = 6:10) %>% \n  tab_options(row_group.background.color = \"grey90\")\n\n\n\n\n\n\n  \n    \n      Wide Familial Distribution\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      adj_r_squared\n      r_squared\n    \n  \n  \n    \n      M. bidentata\n    \n    (Intercept)\n154.179166615\n8.887599e+00\n17.3476733\n4.585745e-19\n0.31631057\n0.3846795\n    date\n0.000420996\n5.107686e-04\n0.8242401\n4.152309e-01\nNA\nNA\n    tot_rain_mm\n0.001982397\n3.135284e-03\n0.6322862\n5.311954e-01\nNA\nNA\n    avg_temp_c\n-0.103431488\n8.261240e-02\n-1.2520093\n2.186401e-01\nNA\nNA\n    avg_ppfd_mmol_m2\n0.001107643\n2.886923e-04\n3.8367595\n4.840707e-04\nNA\nNA\n    \n      D. excelsa\n    \n    (Intercept)\n219.414487153\n3.146641e+01\n6.9729747\n3.562087e-08\n0.04477991\n0.1403019\n    date\n0.003724497\n1.808369e-03\n2.0595893\n4.672527e-02\nNA\nNA\n    tot_rain_mm\n-0.011572595\n1.110043e-02\n-1.0425362\n3.041140e-01\nNA\nNA\n    avg_temp_c\n0.050401737\n2.924880e-01\n0.1723207\n8.641505e-01\nNA\nNA\n    avg_ppfd_mmol_m2\n-0.001100876\n1.022111e-03\n-1.0770616\n2.886176e-01\nNA\nNA"
  },
  {
    "objectID": "blog-posts/2023-12-10-luquillo-drought/index.html#analysis---dynamic-time-series",
    "href": "blog-posts/2023-12-10-luquillo-drought/index.html#analysis---dynamic-time-series",
    "title": "Drought Tolerance in Tropical Tree Species",
    "section": "Analysis - Dynamic Time Series",
    "text": "Analysis - Dynamic Time Series\nClimate impacts on tree growth are likely not immediate, in order to produce a more accurate model it would be best to add a lag to tree growth. Ideally, this regression would test if current diameter at breast height is dependent on past diameter *and* past climate variables.\n\n\nView Code\n# conduct dynamic regression for a single species\n\n# apply function to all species (dynamic time series)\nfor (i in unique(clim_dbh_full$species)) {\n  \n  data <- clim_dbh_full %>% filter(species == i)\n  \n  model <- dynlm(mean_daily_dbh ~ tot_rain_mm + \n                   lag(tot_rain_mm, 1) + \n                   avg_temp_c + \n                   lag(avg_temp_c, 1) + \n                   avg_ppfd_mmol_m2 + \n                   lag(avg_ppfd_mmol_m2, 1), \n                 data = data)\n  \n  tidy_results_fun(model, data)\n}\n\n\nUpon running these models, I recognized they were producing abnormally high values of r-squared (between 0.96 and 1). Inflated values of the adjusted r-squared can be caused by autocorrelation, which violates one of the assumptions that must be met to run a dynamic linear model. The autocorrelation between previous and current diameter breast height was very high, and can be seen by creating an ACF plot:\n\n\nView Code\n# isolate single species for example\ncasarb <- clim_dbh_full %>% filter(species == \"CASARB\")\n\n# plot autocorrelation\nacf(casarb$mean_daily_dbh, lag.max = 12, na.action = na.pass,  main = 'Autocorrelation for CASARB Diameter')"
  },
  {
    "objectID": "blog-posts/2023-12-10-luquillo-drought/index.html#results",
    "href": "blog-posts/2023-12-10-luquillo-drought/index.html#results",
    "title": "Drought Tolerance in Tropical Tree Species",
    "section": "Results",
    "text": "Results\nAfter running several regressions, I found that the addition of climate variables did not improve the model’s predictive capabilities. In fact, the simple time series for C. arborea (CASARB) was the most accurate, with a model that explained 70% of variation in diameter growth (y). CASARB has the largest increase in mean diameter height between the first and last measurement, and I believe this is what produced such a well-performing model. Given more time, I predict that time alone would be a much better indicator of diameter growth than what is demonstrated here. All species except M. bidentata (MANBID) experienced decreases in adjusted r-squared values when climate variables were added. This suggests climate variables caused overfitting, but the one exception is interesting. By adding climate variables, the MANBID model explained 30% of variation in diameter, as opposed to 12% in the initial model. While this percentage is still rather low, it suggests potential discrepancies between which tree species are more rapidly impacted by climate."
  },
  {
    "objectID": "blog-posts/2023-12-10-luquillo-drought/index.html#limitations",
    "href": "blog-posts/2023-12-10-luquillo-drought/index.html#limitations",
    "title": "Drought Tolerance in Tropical Tree Species",
    "section": "Limitations",
    "text": "Limitations\nGiven more time, I would have analyzed the change in diameter at breast height between measurements instead of the diameter itself. This could reduce the impact of autocorrelation, so a dynamic time series could be run instead. Alternatively, an autoregressive moving average (or ARIMA) model would allow me to compare diameters regardless of the high autocorrelation. As mentioned, I suspect that data collected over a longer period would demonstrate a stronger linear relationship between tree diameter and time. To expand this analysis, growth rates during periods of drought could be compared to rates during favorable climate conditions."
  },
  {
    "objectID": "blog-posts/2023-12-10-luquillo-drought/index.html#conclusion",
    "href": "blog-posts/2023-12-10-luquillo-drought/index.html#conclusion",
    "title": "Drought Tolerance in Tropical Tree Species",
    "section": "Conclusion",
    "text": "Conclusion\nMuch more work needs to be done to properly assess the impact that climate change has on tropical rain forests, and how these impacts may be different from temperate forests. Data on trees physiological responses can be much harder to collect, leaving gaps in historic records. Similarly, species distribution data can be hard to difficult consolidate. Still, improvements in technology, climate science, and environmental empathy continue to lead us in the right direction."
  },
  {
    "objectID": "blog-posts/2023-12-10-luquillo-drought/index.html#footnotes",
    "href": "blog-posts/2023-12-10-luquillo-drought/index.html#footnotes",
    "title": "Drought Tolerance in Tropical Tree Species",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEffects of Drought on Forests and Rangelands | Climate Change Resource Center. https://www.fs.usda.gov/ccrc/topics/effects-drought-forests-and-rangelands. Accessed 10 Dec. 2023.↩︎\nRabinowitz, Harold, and Suzanne Vogel, editors. “Chapter 11 - Style and Usage in Earth Science and Environmental Science.” The Manual of Scientific Style, Academic Press, 2009, pp. 427–68. ScienceDirect, https://doi.org/10.1016/B978-012373980-3.50015-0.↩︎"
  },
  {
    "objectID": "blog-posts/2024-06-20-mint-shiny-app/index.html",
    "href": "blog-posts/2024-06-20-mint-shiny-app/index.html",
    "title": "Visualizing My Personal Spending Data Using R-Shiny",
    "section": "",
    "text": "In December 2023 it was announced that the Mint personal finance tool, hosted by Intuit, would no longer be supported. Mint users, such as myself, could download their historical spending data as a CSV file or switch over to Credit Karma. I initially transferred my data to Credit Karma, but I was not thrilled with the user interface. Unsurprisingly the site was focused more on tracking credit history, and I missed the simple interactive plots. In a bid to take control of my personal data, I decided to recreate the old dashboard (below)."
  },
  {
    "objectID": "blog-posts/2024-06-20-mint-shiny-app/index.html#background",
    "href": "blog-posts/2024-06-20-mint-shiny-app/index.html#background",
    "title": "Visualizing My Personal Spending Data Using R-Shiny",
    "section": "",
    "text": "In December 2023 it was announced that the Mint personal finance tool, hosted by Intuit, would no longer be supported. Mint users, such as myself, could download their historical spending data as a CSV file or switch over to Credit Karma. I initially transferred my data to Credit Karma, but I was not thrilled with the user interface. Unsurprisingly the site was focused more on tracking credit history, and I missed the simple interactive plots. In a bid to take control of my personal data, I decided to recreate the old dashboard (below)."
  },
  {
    "objectID": "blog-posts/2024-06-20-mint-shiny-app/index.html#organizing-the-data",
    "href": "blog-posts/2024-06-20-mint-shiny-app/index.html#organizing-the-data",
    "title": "Visualizing My Personal Spending Data Using R-Shiny",
    "section": "Organizing the Data",
    "text": "Organizing the Data\nBefore I could start writing code for the dashboard, I needed to decide how to structure my data. Mint’s tool grouped spending based on categories they provided. Since I had been using Mint for several years, I was familiar with my top spending categories and decided to keep most of the naming conventions. Using an Excel spreadsheet, I created a data table for my transaction history with the following structure:\n\n\n\n\n\n\n\n\n\n\n\n\npurchase_date\nstore\ntotal_cost\ncategory\nsubcategory\nsplit\nnote\n\n\n\n\n2/18/24\nshop1\n12.34\nHome\nSupplies\n0\nNA\n\n\n2/28/24\nshop2\n41.23\nAuto\nGas\n0\nNA\n\n\n3/01/24\nshop1\n10.00\nGift\nDonation\n1\nDonated at grocery store\n\n\n3/01/24\nshop3\n5.92\nGrocery\nGrocery\n1\nNA\n\n\n\nTo prevent errors in the category and subcategory columns, I created a pre-defined drop down list to choose from. This would minimize misspellings and keep capitalization consistent. I added an additional constraint on the date, so I could not create rows for dates that had not yet passed. The split column is a binary to indicate if a single transaction falls under two difference categories (as shown in the Gift-Grocery example). The split and note columns are useful when viewing the raw data, but were removed before the data visualization step."
  },
  {
    "objectID": "blog-posts/2024-06-20-mint-shiny-app/index.html#spending-by-category",
    "href": "blog-posts/2024-06-20-mint-shiny-app/index.html#spending-by-category",
    "title": "Visualizing My Personal Spending Data Using R-Shiny",
    "section": "Spending by Category",
    "text": "Spending by Category\nMy first goal was to recreate the summary of spending by category, as shown in the image above. Mint had the option to toggle between a pie and bar chart, but I decided to focus on the bar chart since it’s easier to visually compare groups. To achieve this, I needed to create a reactive data table that would be filtered by the selected date range and spending categories. This table contains the sums of each spending category, and was used to create the reactive bar chart.\n\nData TableBar Chart\n\n\nReactive data table for spending by category\n\n\nView Code\nspending_by_cat_df &lt;- reactive({\n  \n  # starting with the Excel sheet with spending data\n  spending %&gt;% \n    \n    # filter using start and end date inputs\n    filter(date &gt;= input$cat_bar_chart_input[1] & \n             date &lt;= input$cat_bar_chart_input[2]) %&gt;%\n    \n    # filter using selected spending categories\n    filter(category %in% c(input$cat_df_input)) %&gt;% \n    \n    # calculate total spent for each category between given dates\n    group_by(category) %&gt;% \n    summarise(total_spent = sum(total_cost)) %&gt;% \n    arrange(desc(total_spent))\n  \n})\n\n\n\n\nBar chart for spending by category\n\n\nView Code\noutput$cat_bar_chart_output &lt;- renderPlot({\n  \n  # use reactive data frame for plot\n  ggplot(spending_by_cat_df(), \n         \n         # plot total spent vs. category\n         aes(x = reorder(x = category, X = total_spent), \n             y = total_spent, fill = category)) +\n    geom_col(aes(fill = category), show.legend = FALSE) +\n    \n    # consistent colors regardless of selected categories\n    scale_fill_manual(values = c(\"Shopping\" = \"#D6306D\",\n                                 \"Home\" = \"#72C8FF\",\n                                 \"Grocery\" = \"#7556FF\",\n                                 \"Education\" = \"#FEC009\",\n                                 \"Auto\" = \"#1BC599\", \n                                 \"Gifts\" = \"#FD8588\",\n                                 \"Travel\" = \"#B4A4FE\",\n                                 \"Amusement\" = \"#FD9C4F\",\n                                 \"Health\" = \"#6FF7B4\")) +\n    \n    # add labels for total spent\n    geom_text(aes(label = scales::dollar(round(total_spent, 0)), \n                  hjust = -0.2), size = 5) +\n    scale_y_continuous(labels = scales::label_dollar(scale = 1),\n                       limits = c(0, 1000)) +\n    \n    # plot aesthetics\n    theme_light() +\n    labs(fill = \"Category\", y = \"Total Spent\") +\n    theme(axis.text.y = element_text(size = 16),\n          axis.text.x = element_text(size = 12),\n          axis.title.x = element_text(size = 16),\n          axis.title.y = element_blank(),\n          panel.border = element_blank(),\n          panel.grid.major.y = element_blank()) +\n    coord_flip()\n  \n})\n\n\n\n\n\n\nFinal Product\n\nThe chart is ordered by highest spending, and has labels with rounded totals to summarize top spending categories during the selected time. The data table below provides additional data, including specific stores and subcategories that contribute to the total spent. This is a simple interactive table of the transaction data described in the “Organizing the Data” section."
  },
  {
    "objectID": "blog-posts/2024-06-20-mint-shiny-app/index.html#budget-tab",
    "href": "blog-posts/2024-06-20-mint-shiny-app/index.html#budget-tab",
    "title": "Visualizing My Personal Spending Data Using R-Shiny",
    "section": "Budget Tab",
    "text": "Budget Tab\nMint also provided several tools to indicate whether your spending was within an established budget. Similar to the spending by category, this required a reactive data table that was connected to the visualization. I don’t budget for every spending subcategory, so there were a few more manual steps in creating the associated table. I then created the bar chart using ggplot2, using the percent of budget spent to create a progress bar.\n\nData TableBar Chart\n\n\nReactive data table for subcategory budgets\n\n\nView Code\nbudget_df &lt;- reactive({\n  \n  # starting with the Excel sheet with spending data\n  spending %&gt;% \n    \n    # filter to list of subcategories that do have a set budget\n    filter(subcategory %in% budget_subcategories) %&gt;% \n    \n    # monthly budget based on selection\n    filter(month == input$budget_month_input) %&gt;% \n    \n    # calculate total spent within subcategory\n    mutate(subcategory = as.factor(subcategory)) %&gt;% \n    group_by(subcategory) %&gt;% \n    summarise(total_spent = sum(total_cost)) %&gt;% \n    \n    # define numeric budget for each subcategory\n    mutate(budget = case_when(subcategory == \"Allowance\" ~ 100,\n                              subcategory == \"Grocery\" ~ 320,\n                              subcategory == \"Gas\" ~ 300,\n                              subcategory == \"Coffee\" ~ 25,\n                              subcategory == \"Subscriptions\" ~ 7,\n                              subcategory == \"Insurance\" ~ 183,\n                              subcategory == \"Rent\" ~ 928)) %&gt;% \n    \n    # create categorical column to indicate proximity to budget\n    mutate(over = case_when(budget &lt; total_spent ~ \"Over Budget\",\n                            budget == total_spent ~ \"At Limit\",\n                            budget &gt; total_spent ~ \"On Track\")) %&gt;% \n    \n    # calculate percent of budget spent & remaining monthly balance\n    mutate(percent = round(total_spent/budget * 100),\n           remaining = budget - total_spent)\n})\n\n\n\n\nProgress bar chart for percent of budget spent\n\n\nView Code\noutput$budget_plot_output &lt;- renderPlot({\n  \n  # use reactive data frame for plot\n  ggplot(budget_df(), \n         \n         # plot percent spent for each subcategory\n         aes(x = subcategory, \n             y = percent, \n             fill = over)) +\n    \n    # plain gray bar for background to progress bar\n    geom_col(aes(y = 100), fill = \"grey90\", col = \"black\", width = 0.6) +\n    \n    # progress bars for each subcategory \n    geom_col(width = 0.585, alpha = 0.85) +\n    \n    # color based on spending status \n    scale_fill_manual(values = c(\"At Limit\" = \"#F1AF32\",\n                                 \"On Track\" = \"#438D29\",\n                                 \"Over Budget\" = \"#DA4D49\")) +\n    \n    # plot aesthetics\n    theme_minimal() +\n    labs(fill = \"Status\", y = \"Percent of Monthly Budget\") +\n    scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n    coord_flip(ylim = c(0, 100)) +\n    theme(panel.grid.major.y = element_blank(),\n          axis.text.y = element_text(size = 16), \n          axis.text.x = element_text(size = 12),\n          axis.title.y = element_blank(),\n          axis.title.x = element_text(size = 16),\n          legend.title = element_text(size = 14),\n          legend.text = element_text(size = 12))\n  \n})\n\n\n\n\n\n\nFinal Product\n\n\n\n\n\nThis page utilizes clicks as inputs to return a budget summary for the desired month. Once a subcategory on the chart is clicked, the row below the visualization automatically populates specific values regarding this specific budget. This makes it easy to see the remaining balance for the month."
  },
  {
    "objectID": "blog-posts/2024-06-20-mint-shiny-app/index.html#conclusion",
    "href": "blog-posts/2024-06-20-mint-shiny-app/index.html#conclusion",
    "title": "Visualizing My Personal Spending Data Using R-Shiny",
    "section": "Conclusion",
    "text": "Conclusion\nCompleting this dashboard has given me the opportunity to explore R-Shiny and create a product that’s solely tailored to my needs. I find myself reaching for this tool often and highly recommend everyone take some time to tackle those personal projects you might have on the back burner. The GitHub repository and full code for this project are not publicly available, as it contains some personal information. However, I hope the code provided is enough to inspire anyone to play with R-Shiny."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Briana Barajas",
    "section": "",
    "text": "Welcome 🐛\n\nWelcome to my page! I am a budding environmental data scientist ready to tackle questions about conservation and the ever-changing landscape. Outside of coding, I spending my time bird watching, swimming, and reading."
  },
  {
    "objectID": "blog-posts/2024-03-12-frog-infrographic/index.html",
    "href": "blog-posts/2024-03-12-frog-infrographic/index.html",
    "title": "Creating an infographic using Oregon spotted frog capture data",
    "section": "",
    "text": "The capture-mark-recapture technique (herein CMR) is a common surveying technique used to calculate estimations of populations or survival over time. As with many mathematical analyses, there are some assumptions that much be met to calculate survival using CMR data. The data used for my infographic was used in a publication that commented on the species survival over several years (Rowe et. al). To calculate survival, it is assumed that the likelihood of catching any individual is the same for all individuals in the population. With this in mind, I created several visualizations that answer the question, what factors affect how easy it is to capture a frog?"
  },
  {
    "objectID": "blog-posts/2024-03-12-frog-infrographic/index.html#overarching-question",
    "href": "blog-posts/2024-03-12-frog-infrographic/index.html#overarching-question",
    "title": "Creating an infographic using Oregon spotted frog capture data",
    "section": "",
    "text": "The capture-mark-recapture technique (herein CMR) is a common surveying technique used to calculate estimations of populations or survival over time. As with many mathematical analyses, there are some assumptions that much be met to calculate survival using CMR data. The data used for my infographic was used in a publication that commented on the species survival over several years (Rowe et. al). To calculate survival, it is assumed that the likelihood of catching any individual is the same for all individuals in the population. With this in mind, I created several visualizations that answer the question, what factors affect how easy it is to capture a frog?"
  },
  {
    "objectID": "blog-posts/2024-03-12-frog-infrographic/index.html#about-the-data",
    "href": "blog-posts/2024-03-12-frog-infrographic/index.html#about-the-data",
    "title": "Creating an infographic using Oregon spotted frog capture data",
    "section": "About the Data",
    "text": "About the Data\nFor the final visualizations, I used two different data frames within the same data publication by the United States Geological Survey (USGS). The first data frame contained the data on CMR surveys of the Oregon Spotted frog (Rana pretiosa) from 2009-2021. The raw data was in “long” format, where every year and visit number were an individual column of 0s and 1s, indicating whether or not an individual was captured. Using pivot_longer , I split year and visit number into individual columns. This allowed me to easily sum the data so that I could count the number of frogs detected based on particular groups (reach, sex, and size).\nThe second data frame contained data on environmental variables around the time the surveys were conducted. I only focused on the NDVI column in this data set, as I hypothesized that vegetation density might impact the ease of catching frogs. The U.S. Fish & Wildlife service also hosts a geographic data set with the species range of several threatened, and endangered species. I used the species range data to create a custom range map for the infographic.\n\nRead in data\n\n\nView Code\n## ========================================\n##             Read in Data            ----\n## ========================================\n# set data directory\ndata_dir &lt;- \"/Users/bri_b/Documents/Work/bb-website/bbarajas429.github.io/blog-posts/2024-03-12-frog-infrographic/data\"\n\n# read frog data ----\nfrogs_raw &lt;- read_csv(here(data_dir, \"frog_cmr\", \"cmrData.csv\")) %&gt;% \n  clean_names()\n\n# read water data ----\nenv_raw &lt;- read_csv(here(data_dir, \"frog_cmr\", \"waterCov.csv\"))\n\n# species range data ----\nquery &lt;- \"SELECT * FROM usfws_complete_species_current_range_2 WHERE SCINAME='Rana pretiosa' \"\n\nrange_map &lt;- st_read(here(data_dir, \"usfws_complete_species_current_range\",\n                          \"usfws_complete_species_current_range_2.shp\"),\n                     query = query) %&gt;%\n  st_make_valid() %&gt;%\n  clean_names()\n\n# full state maps ----\nstate_map &lt;- st_read(here(data_dir, \"cb_2018_us_state_500k\", \"cb_2018_us_state_500k.shp\")) %&gt;%\n  st_make_valid() %&gt;%\n  clean_names() %&gt;%\n  filter(name == \"Oregon\" | name == \"Washington\")\n\n# create a coordinate point for data collection area\ndata_location &lt;- data.frame(lat = c(43.224875),\n                            lon = c(-121.587244)) %&gt;%\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = st_crs(state_map))\n\n## ========================================\n##             Read in Images          ----\n## ========================================\n# frog icons ----\nfrog_female &lt;- here(data_dir, \"frog-female.png\")\nfrog_male &lt;- here(data_dir, \"frog-male.png\")\n\n# jack creek map ----\ncreek_img &lt;- png::readPNG(here(data_dir, \"jack-creek-inset.png\"), native = TRUE)\n\n# sul image ----\nsul_img &lt;- png::readPNG(here(data_dir, \"sul-measurement.png\"), native = TRUE)\n\n# lilypad image ----\nyellow_pad &lt;- here(data_dir, \"yellow-pad.png\")\ngreen_pad &lt;- here(data_dir, \"green-pad.png\")\n\n# lilypad legend ----\nlilypad_legend &lt;- png::readPNG(here(data_dir, \"lilypad-legend.png\"))\n\n\n\n\nData Wrangling\n\n\nView Code\n## ========================================\n##        Wrangle environmental data   ----\n## ========================================\nenv &lt;- env_raw %&gt;% \n  \n  # select sites of interest (most surveyed)\n  filter(reach == \"Middle Jack\" | reach == \"Upper Jamison\")\n\n## ========================================\n##          Wrangle frog CMR data      ----\n## ========================================\nfrogs &lt;- frogs_raw %&gt;% \n  \n  # filter to most surveyed reaches\n  filter(reach == \"Middle Jack\" | reach == \"Upper Jamison\") %&gt;% \n  \n  # pivot to split year from detected column\n  pivot_longer(cols = 5:43,\n               names_to = \"year_visit\",\n               values_to = \"frog_detected\") %&gt;% \n  \n  # split year and visit number into two columns %&gt;% \n  separate(year_visit, \n           c(\"year\", \"visit\"),\n           '_') %&gt;% \n  \n  # remove x that precedes the year (x2010, x2011)\n  mutate(year = str_remove(year, 'x')) %&gt;% \n  \n  # rename size to include units\n  rename(sul_mm = sul) %&gt;% \n  \n  # remove years w/no frog surveys at Upper Jamison\n  filter(year %in% c(2009:2019))\n\n\n# clean environment\nrm(env_raw, frogs_raw, query)"
  },
  {
    "objectID": "blog-posts/2024-03-12-frog-infrographic/index.html#creating-data-visualizations",
    "href": "blog-posts/2024-03-12-frog-infrographic/index.html#creating-data-visualizations",
    "title": "Creating an infographic using Oregon spotted frog capture data",
    "section": "Creating Data Visualizations",
    "text": "Creating Data Visualizations\n\nSpecies Range Map\nThe species range map was not one of the key visualizations, and required minimal data wrangling. The USFWS dataset included ranges for all threatened or endangered species, so I used a query to select only the Oregon Spotted Frog range. To better demonstrate the study area, I added an inset map with labeled locations of interest. This map is an image from the original publication and has the site names clearly labeled which would be important for the “vegetation” figure.\n\n\nView Code\n# create species range map\nggplot() +\n  \n  # map states & species range\n  geom_sf(data = state_map, col = \"slategray\") +\n  geom_sf(data = range_map, fill = \"yellowgreen\", col = \"black\") +\n  \n  # add box around study area \n  geom_sf(data = data_location, shape = 15, size = 6, col = \"dodgerblue\",\n          alpha = 0.45) +\n  \n  # add text annotation for study area\n  annotate(geom = \"text\", x = -118, y = 45.4, label = \"Study Area \\n Jack Creek, Oregon\", family = \"noto\", size = 4, col = \"black\") +\n  \n  # expand axis limits so inset image does not get cropped\n  coord_sf(xlim = c(-125, -116), ylim = c(41.5, 49.5), expand = FALSE) +\n  \n  # add lines connecting study area to inset map\n  geom_curve(aes(x = -120, xend = -121.50,\n                 y = 44.98, yend = 43.224875),\n             curvature = 0, col = \"black\", linewidth = 0.7) +\n  \n  geom_curve(aes(x = -120, xend = -121.50,\n                 y = 43.02, yend = 43.224875),\n             curvature = 0, col = \"black\", linewidth = 0.7) +\n  \n  # add map of study area\n  annotation_raster(creek_img, xmax = -120, xmin = -116,\n                    ymax = 45, ymin = 43) +\n  \n  # update general theme to remove background\n  theme_void() +\n  \n  # add title\n  labs(title = \"Species Range Map\") +\n  \n  # adjust title theme and size\n  theme(plot.title = element_text(hjust = 0.5, vjust = 0, \n                                  family = \"roboto\", size = 30))\n\n\n\n\n\n\n\n\n\n\n\nMales vs. Females\nSince I was comparing counts, I decided I would create a unique version of a bar chart. I was originally looking at lollipop charts when I got the idea to connect the point to a curved line, so it appeared as though the frog was jumping. Since the data presented in this plot was so simple, I wanted to remove as many of the plot elements as possible. For example, instead of a legend I directly wrote out the frog counts by each point. I also made the male and female frog different colors, which I kept consistent in the final inforgraphic caption. Additionally, after calculating the sum I changed the sex column to be the same character string so both frogs would both be on the same axis.\n\n\nView Code\n## ............. Data Preparation..................\n# create data subset of male vs. female frogs captured\nmf_count &lt;- frogs %&gt;% \n  group_by(sex) %&gt;% \n  summarise(frog_catch = sum(frog_detected))\n\n# add column with male/female frog images\nmf_count$image &lt;- c(frog_female, frog_male)\n\n# change sex to same value so frogs can be on the same line\nmf_count$sex &lt;- \"A\"\n\n## ..................Plot..........................\n\n# create plot of male vs. female frogs caught\nggplot(data = mf_count) +\n  \n  # add images of frogs for male and female\n  geom_image(aes(x = frog_catch, y = sex, image = image), \n             size = 0.2) +\n  \n  # add hop line for males\n  geom_curve(aes(x = 0, xend = 267, y = 1, yend = 1), linetype = 2,\n             curvature = -0.4, col = \"#18BA9A\", linewidth = 1) +\n  \n  # add hop line for females\n  geom_curve(aes(x = 0, xend = 350, y = 1, yend = 1), linetype = 2,\n             curvature = -0.4, col = \"#754edb\", linewidth = 1) +\n  \n  # expand x-axis to add space for text\n  coord_cartesian(xlim = c(0, 360)) +\n  \n  # pre-set theme\n  theme_minimal() +\n  \n  # customize labels and title\n  labs(title = \"Sex of Captured Frogs\") +\n  \n  # add labels for data points\n  annotate(geom = \"text\", x = 342.5, y = 0.87, label = \"350 female frogs\", \n           family = \"noto\", size = 4, col = \"#754edb\") +\n  annotate(geom = \"text\", x = 259, y = 0.87, label = \"267 male frogs\",\n           family = \"noto\", size = 4, col = \"#18BA9A\") +\n  \n  # remove gridlines\n  theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        \n        # remove labels that aren't needed\n        axis.text.x = element_blank(),\n        axis.title.y = element_blank(),\n        \n        # customize fonts\n        plot.title = element_text(family = \"roboto\", size = 30,\n                                  hjust = 0.5, vjust = -9.5),\n        axis.text.y = element_blank(),\n        axis.title.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nSize Distribution\nSince frog size is a continuous variable, I wanted to focus on which sizes were most common instead of calculating totals. Ideally, the distribution of frogs caught would be normally distributed, so I used arrows and notes to highlight where this was not the case. I chose the green color to continue the frog theme, and kept all my annotations black so they popped against the white background. I decided to keep the gridlines so the two peaks could be easily compared. Different species have different standards for being measured, so I wanted to include an image that shows how the frogs were measured. Frogs are measured from snout to urostyle. Instead of defining urostyle, I summarized this measurement as “length” for simplicity.\n\n\nView Code\n## ............. Data Preparation..................\n# subset to only include frogs that were captured\nfrogs_subset &lt;- frogs %&gt;% \n  filter(frog_detected == 1)\n\n## ..................Plot..........................\n# plot density of frog size\nggplot(frogs_subset, aes(x = sul_mm)) +\n  geom_density(fill = \"#4EA72E\", col =\"seagreen\", alpha = 0.8) +\n  \n  # change standard theme\n  theme_minimal() +\n  \n  # update axis titles\n  labs(y = \"Density\", \n       x = \"Length (mm)\",\n       title = \"Distribution of Frog Length\") +\n  \n  # update text font and size\n  theme(axis.title = element_text(family = \"noto\", size = 16),\n        axis.text = element_text(family = \"noto\", size = 14),\n        plot.title = element_text(family = \"roboto\",\n                                  hjust = 0.5, size = 30, vjust = 1),\n        \n        # increase plot margin\n        plot.margin = margin(1,0,0,0, \"cm\")) +\n  \n  # update plot to start at y-axis\n  scale_y_continuous(expand=c(0, 0)) +\n  \n  # add arrow pointing to most common sizes\n  geom_curve(aes(x = 47.6, xend = 53,\n                 y = 0.047, yend = 0.045),\n             curvature = 0.2, arrow = grid::arrow()) +\n  \n  # add note on common frog sizes\n  annotate(geom = \"text\", family = \"noto\", size = 3,\n           label = \"Most captured frogs\\n were between 53 - 55 mm \\n in length\",\n           x = 46, y = 0.051) +\n  \n  # add image for measuring frog lengths\n  annotation_raster(sul_img, xmax = 82, xmin = 72,\n                    ymax = 0.052, ymin = 0.033)\n\n\n\n\n\n\n\n\n\n\n\nVegetation\nFor vegetation, I wanted to highlight several parts of the data including year, average summer normalized difference vegetation index (NDVI), and frog count. To avoid overly complex figures, I decided to focus only on NDVI over time and compare frog counts directly within the graphic. I originally created a line graph that demonstrated NDVI over time for both sites. I decided against this, because I was more interested in displaying the difference not changes. Instead of the line graph, I opted for a dumbell plot to better demonstrate theses differences. I wanted the data points to look like lilypads in a pond. Since Middle Jack has more dense vegetation (higher NDVI), I made the leaf more green than the one for Upper Jamison. I also know that NDVI is not an intuitive variable, so I added a line at 0 and text highlighting that values above zero indicate more dense vegetation.\n\n\nView Code\n## ............. Data Preparation..................\n# prepare NDVI data for dumbell plot\nenv_db_data &lt;- env %&gt;% \n  filter(year &lt;= 2019) %&gt;% \n  select(c(\"year\", \"mdNDVI\", \"reach\")) %&gt;% \n  pivot_wider(names_from = reach, values_from = mdNDVI) %&gt;% \n  clean_names() %&gt;% \n  mutate(year = as.factor(year),\n         leaf = green_pad,\n         dry_leaf = yellow_pad)\n\n## ..................Plot..........................\n# create dumbell plot of annual NDVI\nggplot(env_db_data) +\n  \n  # add line for 0 axis\n  geom_hline(yintercept = 0, linetype = 3) +\n  \n  # add lines to connect NDVI of different sites\n  geom_segment(aes(y = middle_jack, yend = upper_jamison,\n                   x = year, xend = year), \n               \n               linewidth = 0.7, col = \"slategray\") +\n  \n  # add points as images\n  geom_image(aes(x = year, y = middle_jack,\n                 image = leaf), size = 0.08) +\n  geom_image(aes(x = year, y = upper_jamison,\n                 image = dry_leaf), size = 0.08) +\n  \n  # change to standard theme\n  theme_minimal() +\n  \n  # update axis names and titles\n  labs(y = \"NDVI\",\n       x = \"Year\",\n       title = \"Average Summer NDVI\") +\n  \n  # update theme\n  # update background and grid color\n  theme(panel.background = element_rect(fill = \"aliceblue\",\n                                        color = \"lightblue3\",\n                                        linewidth = 1),\n        panel.grid.major = element_line(color = \"azure2\"),\n        panel.grid.minor = element_line(color = \"azure2\"),\n        \n        # update fonts and text size\n        plot.title = element_text(family = \"roboto\", size = 30, \n                                  hjust = 0.5),\n        axis.title = element_text(family = \"noto\", size = 16),\n        axis.text = element_text(family = \"noto\", size = 14)) +\n  \n  # add annotation to provide information for NDVI\n  annotate(geom = \"text\", size = 4, family = \"noto\",\n           label = \"Dense vegetation \\n Sparse vegetation\",\n           x = 7, y = 0, col = \"blue\") +\n  \n  # add legend \n  annotation_raster(lilypad_legend, xmin = 8.7, xmax = 11,\n                    ymin = 2, ymax = 1.1) +\n  \n  # increase x-axis length\n  coord_cartesian(xlim = c(0,11)) +\n  \n  # increase spacing between x-axis ticks\n  scale_x_discrete(expand = c(0, -11))"
  },
  {
    "objectID": "blog-posts/2024-03-12-frog-infrographic/index.html#infographic",
    "href": "blog-posts/2024-03-12-frog-infrographic/index.html#infographic",
    "title": "Creating an infographic using Oregon spotted frog capture data",
    "section": "Infographic",
    "text": "Infographic\nAfter creating my plots in R, I switched over to Canva to create the final infographic. Unfortunately, I realized that I had not considered some of the requirements for exporting images using ggsave. Instead of updating the code, I decided to adjust some of the final titles and images in Canva. Since my overarching question is not clear using visuals alone, I started my infographic with a short background section."
  },
  {
    "objectID": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html",
    "href": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html",
    "title": "Air Quality Assesment of California Thomas Fire",
    "section": "",
    "text": "The Thomas Fire began late December (2017) in Ventura County and continued to affect both Santa Barbara and Ventura counties into the new year. This analysis uses raster data to produce a map of the fire’s perimeter. Additionally, a simple visualization was created to demonstrated the severity of the Thomas Fire’s affect on air quality (AQI) in both counties."
  },
  {
    "objectID": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#background",
    "href": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#background",
    "title": "Air Quality Assesment of California Thomas Fire",
    "section": "",
    "text": "The Thomas Fire began late December (2017) in Ventura County and continued to affect both Santa Barbara and Ventura counties into the new year. This analysis uses raster data to produce a map of the fire’s perimeter. Additionally, a simple visualization was created to demonstrated the severity of the Thomas Fire’s affect on air quality (AQI) in both counties."
  },
  {
    "objectID": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#about-data",
    "href": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#about-data",
    "title": "Air Quality Assesment of California Thomas Fire",
    "section": "About Data",
    "text": "About Data\n\nRaster Data\nThis dataset, from the Landsat Collection 2 Level-2, contains red, green, near-infrared, and shortwave infrared layers. The data was originally collected by the Landsat 8 satellite, and was corrected for surface reflectance and simplified.\nUseful Links:\n\nBand designations for Landsat satellites\nCommon Landsat band combos\nScale factor with Landsat Level-2 science products\nFinal process collection from Microsoft Planetary Computer\n\n\n\nFire Perimeter Data\nCAL Fire provides annual geographic data on fire perimeters. The fire started December 4th, 2017 and was contained by January 12th, 2018. Since the majority of fire growth was in 2017, this analysis will use only the 2017 data.\nUseful Links: Fire perimeter\n\n\nAir Quality Data\nThe Air Quality Data can be pulled from the U.S. Environmental Protection Agency (EPA). This dataset provides air quality data from counties throughout the United States and can be filtered to examine Santa Barbara County during the duration of the Thomas Fire.\nUseful Links: Air quality\n\n\nCounty Perimeter Data\nCalifornia provides shapefiles on all counties within the state.\nUseful Links: County perimeters"
  },
  {
    "objectID": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#libraries-and-functions",
    "href": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#libraries-and-functions",
    "title": "Air Quality Assesment of California Thomas Fire",
    "section": "Libraries and Functions",
    "text": "Libraries and Functions\nLoading necessary libraries and functions:\n\n\nView Code\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches \n\nimport xarray as xr\nimport rioxarray as rioxr\nimport geopandas as gpd\nimport contextily as cx"
  },
  {
    "objectID": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#loading-data",
    "href": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#loading-data",
    "title": "Air Quality Assesment of California Thomas Fire",
    "section": "Loading Data",
    "text": "Loading Data\nThe Landsat raster data and fire perimeter data are stored in a folder and accessed directly. The AQI data can be accessed using the link.\nLoading data for analysis:\n\n\nView Code\n# raster data\nlandsat_raw = rioxr.open_rasterio(os.path.join(os.getcwd(),'data','landsat8-2018-01-26-sb-simplified.nc'))\n\n# ca fire perimeter data\nfire_perimeter = gpd.read_file(os.path.join(os.getcwd(), 'data', 'fire_perim',\n                                           'California_Fire_Perimeters_2017.shp'))\n#import 2017 and 2018 aqi data \naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip')\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip')"
  },
  {
    "objectID": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#data-cleaning-for-visualization",
    "href": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#data-cleaning-for-visualization",
    "title": "Air Quality Assesment of California Thomas Fire",
    "section": "Data Cleaning for Visualization",
    "text": "Data Cleaning for Visualization\n\nRaster Data\nThe raster data contains additional bands that are not needed for this analysis, which will utilize the short-wave infrared (swir22), near infrared (nir08), and red bands. Removing the bands and coordinates which are not needed will improve processing in the following steps.\n\n\nView Code\n# remove extra dims and coords\nlandsat = landsat_raw.squeeze().drop('band')\n\n\n\n\nFire Perimeter Data\nAs mentioned in the, “About Data” section, the fire perimeter data is for the entire state of California. The data was filtered to the fire of interest, and the coordinate reference system (CRS) was updated for mapping purposes.\n\n\nView Code\n# lowercase col names\nfire_perimeter.columns = fire_perimeter.columns.str.lower()\n\n# update CRS to match landsat data\nfire_perimeter = fire_perimeter.to_crs(landsat.rio.crs)\n\n# filter to fire of interest\nthomas = fire_perimeter[fire_perimeter['fire_name']=='THOMAS']"
  },
  {
    "objectID": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#mapping-affected-counties",
    "href": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#mapping-affected-counties",
    "title": "Air Quality Assesment of California Thomas Fire",
    "section": "Mapping Affected Counties",
    "text": "Mapping Affected Counties\nBoth Ventura and Santa Barbara counties were impacted by the 2017 Thomas Fire. The following map shows the extent of both counties\n\n\nView Code\nca_counties = gpd.read_file(os.path.join(os.getcwd(), 'data', 'county-lines','CA_Counties_TIGER2016.shp'))\nimpacted_counties = ca_counties[(ca_counties.NAME == 'Ventura') | (ca_counties.NAME == 'Santa Barbara')]\n\n\n\n\nView Code\nfig, ax = plt.subplots() #create figure\n\nimpacted_counties.to_crs(epsg=3857).plot(ax = ax, figsize = (5,5), alpha = 0.3,\n                                        column = 'NAME', edgecolor='k',\n                                        legend = True,\n                                         legend_kwds={'loc': 'lower right'})\n\ncx.add_basemap(ax, source=cx.providers.Esri.NatGeoWorldMap)\n\n# update title and axes\nax.set_title(\"Counties Impacted by Thomas Fire\", fontdict={\"fontsize\": \"16\"})\nax.set_axis_off()\nplt.savefig('images/impacted-counties.png')"
  },
  {
    "objectID": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#mapping-the-fire-perimeter",
    "href": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#mapping-the-fire-perimeter",
    "title": "Air Quality Assesment of California Thomas Fire",
    "section": "Mapping the Fire Perimeter",
    "text": "Mapping the Fire Perimeter\nThe cleaned and filtered data can be used to produce a false color image depicting the extent of the Thomas Fire’s perimeter:\n\n\nView Code\nfig, ax = plt.subplots()\nax.axis('off')\n\n# map thomas fire perimeter\nthomas.plot(ax=ax, color = 'None', edgecolor = 'red', legend=True)\nthomas_patch = mpatches.Patch(color = 'red', label = 'Fire Perimeter') #legend parameters\n\n# map raster data\nlandsat[['swir22','nir08','red']].to_array().plot.imshow(robust=True) #plot SB county \n\n# update legend and title\nax.set_title('Southern California, Thomas Fire (2017-18)')\n_ = ax.legend(handles=[thomas_patch])\nplt.savefig('images/false-color-image.png')"
  },
  {
    "objectID": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#analyzing-impact-on-aqi",
    "href": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#analyzing-impact-on-aqi",
    "title": "Air Quality Assesment of California Thomas Fire",
    "section": "Analyzing Impact on AQI",
    "text": "Analyzing Impact on AQI\nThe Thomas Fire burned 281,893 acres, affecting both Santa Barbara and Ventura Counties. The map above shows the perimeter of the fire, but doesn’t give much more information on the impact. The following analysis looks at air quality (AQI) in Ventura and Santa Barbara county from 2017 to 2018.\n\nData Cleaning for AQI\nAir quality data is provided for all counties and states within the U.S., therefore filtering is required in addition to data tidying to create subsets of the counties of interest. Additional tidying steps on the date column will be useful for plotting the data.\n\n\nView Code\n# combine 2017-18 AQI data\naqi = pd.concat([aqi_17, aqi_18])\n\n# clean col names\naqi.columns = aqi.columns.str.lower().str.replace(' ','_')\n\n# SB and VC county subset\naqi_sb = aqi.loc[aqi['county_name']=='Santa Barbara',\n       ['date','aqi','category','defining_parameter', 'defining_site','number_of_sites_reporting']]\n\naqi_vc = aqi.loc[aqi['county_name']=='Ventura',\n       ['date','aqi','category','defining_parameter', 'defining_site','number_of_sites_reporting']]\n\n# change date col to data time object\naqi_sb.date = pd.to_datetime(aqi_sb.date)\naqi_vc.date = pd.to_datetime(aqi_vc.date)\n\n# set date column as index\naqi_sb = aqi_sb.set_index('date')\naqi_vc = aqi_vc.set_index('date')\n\n# add column w/5-day rolling average\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()\naqi_vc['five_day_average'] = aqi_vc.aqi.rolling('5D').mean()"
  },
  {
    "objectID": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#plot-aqi-from-2017-2018",
    "href": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#plot-aqi-from-2017-2018",
    "title": "Air Quality Assesment of California Thomas Fire",
    "section": "Plot: AQI From 2017-2018",
    "text": "Plot: AQI From 2017-2018\nThe following plots show air quality in Santa Barbara and Ventura counties, which were both impacted by the Thomas Fire. In addition to the daily aqi (blue), there is a 5-day rolling average that demonstrates the mean AQI over 5-days.\n\n\nView Code\nfig, [ax1, ax2] = plt.subplots(ncols=1, nrows=2,\n                               sharex = True, sharey=True,\n                              figsize = (11,9))\n\naqi_sb.aqi.plot(ax = ax1, color = '#1177A0', legend = True)  \naqi_sb.five_day_average.plot(ax=ax1, color = '#A01177', \n                            title = 'Santa Barbara Air Quality 2017-18', \n                            legend = True, \n                             xlabel = \"Date\", \n                             ylabel = \"AQI\") \n\naqi_vc.aqi.plot(ax = ax2, color = '#1177A0', legend = False)  \n_ = aqi_vc.five_day_average.plot(ax=ax2, color = '#A01177', \n                            title = 'Ventura Air Quality 2017-18',\n                            legend = False, \n                             xlabel = \"Date\",\n                             ylabel = \"AQI\")\nplt.savefig('images/aqi-plot.png')"
  },
  {
    "objectID": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#references",
    "href": "blog-posts/2023-12-13-thomas-fire-perimeter/index.html#references",
    "title": "Air Quality Assesment of California Thomas Fire",
    "section": "References",
    "text": "References\nCalifornia Open Data Portal (2016), CA Geographic Boundaries [Data File] Available from: https://data.ca.gov/dataset/ca-geographic-boundaries. Access date: December 13, 2023.\nCalifornia State Geoportal. (2023, August 7), California Fire Perimeters (all) [Data file] Available from: https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about. Access date: November 28, 2023.\nEarth Resources Observation And Science (EROS) Center. Collection-2 Landsat 8-9 OLI (Operational Land Imager) and TIRS (Thermal Infrared Sensor) Level-2 Science Products. U.S. Geological Survey, 2013, doi:10.5066/P9OGBGM6.\nEnvironmental Protection Agency (EPA). (2023, November 9), Air Data: Air Quality Data Collected at Outdoor Monitors Across the US. Pre-Generated Data Files [URL] Available from: https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI. Access date: November 28, 2023.\n“Thomas Fire.” Wikipedia, Wikimedia Foundation, 9 Nov. 2023, en.wikipedia.org/wiki/Thomas_Fire. Accessed 28 Nov. 2023."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I graduated from the University of California, Santa Barbara in 2022 with a B.S. in Zoology. During that time, I assisted in a meta-analysis, extracting global data on ungulate diets to better understand how plant biodiversity is affected by select functional traits of large herbivores. After graduating, I held various positions within the public sector, working for the National Parks Service and later, the California Department of Food and Agriculture.\n\nI went on to pursue a Master of Environmental Data Science at the UC Santa Barbara, Bren School of Environmental Science & Management. Moving forward, I hope to use spatial data to assess landscape changes, and the impact of ecosystem stressors including climate change, disease, and invasive species."
  },
  {
    "objectID": "about.html#professional-background",
    "href": "about.html#professional-background",
    "title": "About Me",
    "section": "",
    "text": "I graduated from the University of California, Santa Barbara in 2022 with a B.S. in Zoology. During that time, I assisted in a meta-analysis, extracting global data on ungulate diets to better understand how plant biodiversity is affected by select functional traits of large herbivores. After graduating, I held various positions within the public sector, working for the National Parks Service and later, the California Department of Food and Agriculture.\n\nI went on to pursue a Master of Environmental Data Science at the UC Santa Barbara, Bren School of Environmental Science & Management. Moving forward, I hope to use spatial data to assess landscape changes, and the impact of ecosystem stressors including climate change, disease, and invasive species."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nMaster of Environmental Data Science - University of California, Santa Barbara (2024)\nB.S. Zoology - University of California, Santa Barbara (2022)"
  },
  {
    "objectID": "about.html#bird-watching",
    "href": "about.html#bird-watching",
    "title": "About Me",
    "section": "Bird Watching",
    "text": "Bird Watching\nOne of my favorite undergraduate classes was my vertebrate lab. I had the opportunity to visit sites across Santa Barbara to go bird watching, and learn about wildlife surveying techniques. I have continued to bird watch ever since, and keep track of my sightings using the Merlin and eBird apps, which are maintained by the Cornell Lab of Ornithology.\nI thought it would be fun to visualize my personal bird sightings using a leaflet map:\n\n\nView Code\n# add CRS to lat/long points\nbirds &lt;- birds %&gt;%   st_as_sf(coords = c('longitude', 'latitude'), crs = 'EPSG:4236')\n\n# create map\nbirds %&gt;% \n  leaflet() %&gt;% \n  addProviderTiles(providers$Jawg.Streets,\n                   options = providerTileOptions(accessToken = token)) %&gt;% \n  addMarkers(label = birds$common_name,\n             popup = paste0(birds$common_name, ' (',\n                            birds$scientific_name, ')'),\n             clusterOptions = markerClusterOptions(),\n             icon = makeIcon(iconUrl = iconpath,\n                             iconWidth = 32, iconHeight = 32))"
  }
]